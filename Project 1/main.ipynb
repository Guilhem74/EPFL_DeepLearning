{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "from torch import optim\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import dlc_practical_prologue      \n",
    "size=1000;\n",
    "train_input, train_target, train_classes, test_input, test_target, test_classes = \\\n",
    "    dlc_practical_prologue.generate_pair_sets(size)\n",
    "train_input, train_target, train_classes = Variable(train_input), Variable(train_target), Variable((train_classes))\n",
    "test_input, test_target, test_classes = Variable(test_input), Variable(test_target), Variable(test_classes)\n",
    "mini_batch_size = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    Train_classes_One_Hot_IM1=dlc_practical_prologue.convert_to_one_hot_labels(train_input.narrow(1,0,1), train_classes.narrow(1,0,1))\n",
    "#    Train_classes_One_Hot_IM2=dlc_practical_prologue.convert_to_one_hot_labels(train_input.narrow(1,1,1), train_classes.narrow(1,1,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to implement a deep network such that, given as input a series of 2×14×14 tensor, corresponding to pairs of 14 × 14 grayscale images, it predicts for each pair if the first digit is lesser or equal to the second.\n",
    "\n",
    "The training and test set should be 1, 000 pairs each, and the size of the images allows to run experiments rapidly, even in the VM with a single core and no GPU.\n",
    "\n",
    "You can generate the data sets to use with the function generate_pair_sets(N) defined in the file dlc_practical_prologue.py.\n",
    "\n",
    "|      Name     | Tensor Dimension |   Type  |                  Content                 |\n",
    "|:-------------:|:----------------:|:-------:|:----------------------------------------:|\n",
    "|  Train_input  |  N x 2 x 14 x 14 | float32 |                  Images                  |\n",
    "|  Train_target |         N        |  int64  |          Class to predict €{0,1}         |\n",
    "| Train_classes |       N x 2      |  int64  | Classes of the two digits € {0, ... ,9 } |\n",
    "|   Test_input  |  N x 2 x 14 x 14 | float32 |                  Images                  |\n",
    "|  Test_target  |         N        |  int64  |          Class to predict €{0,1}         |\n",
    "|  Test_classes |       N x 2      |  int64  | Classes of the two digits € {0, ... ,9 } |\n",
    "\n",
    "The goal of the project is to compare different architectures, and assess the performance improvement that can be achieved through weight sharing, or using auxiliary losses.\n",
    "\n",
    "For the latter, the training can in particular take advantage of the availability of the classes of the two digits in each pair, beside the Boolean value truly of interest.\n",
    "\n",
    "All the experiments should be done with 1, 000 pairs for training and test. A convnet with ∼ 70, 000 parameters can be trained with 25 epochs in the VM in less than 2s and should achieve ∼ 15% error rate.\n",
    "\n",
    "Performance estimates provided in your report should be estimated through 10+ rounds for each architecture, where both data and weight initialization are randomized, and you should provide estimates of standard deviations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model sharing weights + boolean comparison at the end :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Weight_Sharing_Model(nn.Module):\n",
    "    def __init__(self, nb_hidden):\n",
    "        super(Weight_Sharing_Model, self).__init__()\n",
    "        \"\"\"Modele 1 for digit recognition\"\"\"\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.Layer1 = nn.Linear(1024, nb_hidden)\n",
    "        self.Layer2 = nn.Linear(nb_hidden, 10)\n",
    "        \"\"\"Modele 2 for digit comparison\"\"\"\n",
    "    def forward(self,input_):\n",
    "        img1=input_.narrow(1, 0,1);\n",
    "        img2=input_.narrow(1, 1,1);\n",
    "        x=F.relu(F.max_pool2d(self.conv1(img1), kernel_size=2, stride=1))\n",
    "        x=F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=1))\n",
    "        x = F.relu(self.Layer1(x.view(-1, 1024)))\n",
    "        x = self.Layer2(x)\n",
    "        y=F.relu(F.max_pool2d(self.conv1(img2), kernel_size=2, stride=1))\n",
    "        y=F.relu(F.max_pool2d(self.conv2(y), kernel_size=2, stride=1))\n",
    "        y = F.relu(self.Layer1(y.view(-1, 1024)))\n",
    "        y = self.Layer2(y)\n",
    "        Classe_X = (torch.argmax(x,1))\n",
    "        Classe_Y = (torch.argmax(y,1))\n",
    "        z=Classe_X>Classe_Y\n",
    "        return   x,y, z \n",
    "def train_model(model, train_input_, train_target_):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr = 1e-3)\n",
    "    nb_epochs = 25\n",
    "\n",
    "    for e in range(nb_epochs):\n",
    "        sum_loss=0\n",
    "        for b in range(0, train_input_.size(0), mini_batch_size):\n",
    "            output = model(train_input_.narrow(0, b, mini_batch_size))\n",
    "            loss = criterion(output[0], train_target_.narrow(0, b, mini_batch_size).narrow(1,0,1).view(-1))#Loss for Img1\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "def compute_nb_errors(model, data_input, data_target):\n",
    "    nb_data_errors = 0\n",
    "    for b in range(0, data_input.size(0), mini_batch_size):\n",
    "        output = model(data_input.narrow(0, b, mini_batch_size))\n",
    "        _, predicted_classes = torch.max(output[0].data, 1)#Display error for Img1\n",
    "        for k in range(mini_batch_size):\n",
    "            if data_target.data[b + k] != predicted_classes[k]:\n",
    "                nb_data_errors = nb_data_errors + 1\n",
    "    return nb_data_errors    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyse before and after training :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Weight_Sharing_Model(200);\n",
    "output = model(train_input.narrow(0, 0, 1000))\n",
    "_, predicted_classes = torch.max(output[0].data, 1)#Display error for Img1\n",
    "#print(predicted_classes)\n",
    "print(predicted_classes)\n",
    "\n",
    "train_model(model, train_input, train_classes)\n",
    "output = model(train_input.narrow(0, 0, 1000))\n",
    "_, predicted_classes = torch.max(output[0].data, 1)#Display error for Img1\n",
    "#return   x3,y, z,img1, x2,x3,self.Layer1 \n",
    "#         0  1  2  3    4   5  6\n",
    "print(predicted_classes)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To test and measure error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train_error 2.70% test_error 12.50%\n",
      " train_error 87.00% test_error 88.60%\n",
      " train_error 28.10% test_error 30.60%\n",
      " train_error 79.60% test_error 84.50%\n",
      " train_error 91.80% test_error 90.50%\n",
      " train_error 91.80% test_error 90.50%\n"
     ]
    }
   ],
   "source": [
    "for std in [ -1, 1e-3, 1e-2, 1e-1, 1e-0, 1e1 ]:\n",
    "\n",
    "    model=Weight_Sharing_Model(200);\n",
    "\n",
    "    \n",
    "    if std > 0:\n",
    "            for p in model.parameters(): p.data.normal_(0, std)\n",
    "    train_model(model, train_input, train_classes)\n",
    "    print(' train_error {:.02f}% test_error {:.02f}%'.format(\n",
    "        compute_nb_errors(model,train_input,train_classes.narrow(1,0,1).view(-1).long()) / train_input.size(0) * 100,\n",
    "        compute_nb_errors(model,test_input,test_classes.narrow(1,0,1).view(-1).long()) / test_input.size(0) * 100\n",
    "    )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Size of each layer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 5, 5])\n",
      "torch.Size([32])\n",
      "torch.Size([64, 32, 5, 5])\n",
      "torch.Size([64])\n",
      "torch.Size([200, 1024])\n",
      "torch.Size([200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for k in model.parameters():\n",
    "    print(k.size())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To display an image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADGNJREFUeJzt3W+snnV9x/H3x5ZSWyVtQQ20xELCcIQ4IR0CLs5QzcqfUB4sBhymm2Q82FQ0JAgjmdmDJUtgRJMZFBBlg8ADhEkIMJqCMU7syr8hUISKDCrFdgHF4Z+2+N2DczfpDtB293Xd1znt7/1KTu77us71O9/vOTmf87vu677v80tVIak9b5vpBiTNDMMvNcrwS40y/FKjDL/UKMMvNcrwS40y/FKjDL/UqLlDFpuXg2s+C4csKTXlN7zG9vpt9uXYQcM/n4V8MCuHLCk1ZX2t2+djPe2XGmX4pUZ1Cn+SVUl+lGRTkkv7akrS5I0d/iRzgK8ApwPHAeclOa6vxiRNVpeZ/yRgU1U9W1XbgVuA1f20JWnSuoR/KfDCbtubR/sk7Qe6PNX3Zs8lvuHfAiW5ELgQYD4LOpST1KcuM/9m4MjdtpcBL04/qKquqaoVVbXiIA7uUE5Sn7qEfwNwTJKjkswDzgXu6KctSZM29ml/Ve1M8mng34A5wPVV9URvnUmaqE4v762qu4C7eupF0oB8hZ/UKMMvNcrwS40y/FKjDL/UKMMvNcrwS40y/FKjDL/UKMMvNcrwS40y/FKjDL/UKMMvNcrwS40y/FKjDL/UKMMvNcrwS40y/FKjDL/UKMMvNcrwS43qskT3kUnuT7IxyRNJLuqzMUmT1WXRjp3AxVX1cJJ3Ag8lWVtVT/bUm6QJGnvmr6otVfXw6P4vgY24RLe03+i0XNcuSZYDJwDr3+RzLtEtzUKdL/gleQfwLeBzVfXq9M+7RLc0O3UKf5KDmAr+TVV1Wz8tSRpCl6v9Ab4ObKyqq/prSdIQusz8HwI+CZyW5NHRxxk99SVpwsa+4FdV3wPSYy+SBuQr/KRGGX6pUb08z689e+7vT+k0/mvnfm3ssUcf9IZnX/9fnt1xSKfxD/96+dhjr77rTzrVPvqSBzqNP9A580uNMvxSowy/1CjDLzXK8EuNMvxSowy/1CjDLzXK8EuNMvxSowy/1CjDLzXK8EuNMvxSo1JVgxU7JEvqg1k5WL3Z4ulr/3CmW5gx1512/dhjPzJ/R6faZyw9sdP4/dH6Wser9fI+/YctZ36pUYZfapThlxpl+KVG9bFc15wkjyS5s4+GJA2jj5n/IqZW6JW0H+m6Vt8y4Ezgun7akTSUrjP/l4BLgN+91QFJLkzyYJIHd/DbjuUk9aXLQp1nAVur6qE9HecS3dLs1HWhzrOTPAfcwtSCnTf20pWkiRs7/FV1WVUtq6rlwLnAfVV1fm+dSZoon+eXGtXLWn1V9R3gO318LUnDcOaXGmX4pUa5RPcAfu8vN8x0C2Obs3hxp/E33n3q2GMv2XZ4p9qH8XSn8Qc6Z36pUYZfapThlxpl+KVGGX6pUYZfapThlxpl+KVGGX6pUYZfapThlxpl+KVGGX6pUYZfapRv6T3AzV16RKfxy25/pdP4jx/6H2OPveIzn+hUe7jF5/dPzvxSowy/1CjDLzXK8EuN6rpQ56IktyZ5KsnGJKf01Zikyep6tf/LwD1V9adJ5gELeuhJ0gDGDn+SQ4APA38OUFXbge39tCVp0rqc9h8NbAO+keSRJNclWTj9IJfolmanLuGfC5wIXF1VJwCvAZdOP8gluqXZqUv4NwObq2r9aPtWpv4YSNoPdFmi+yXghSTHjnatBJ7spStJE9f1av9ngJtGV/qfBf6ie0uShtAp/FX1KLCip14kDchX+EmNMvxSo3w//wFux3vf1Wn82Uvu6zT+h785cuyxb3vupU61X+80+sDnzC81yvBLjTL8UqMMv9Qowy81yvBLjTL8UqMMv9Qowy81yvBLjTL8UqMMv9Qowy81yvBLjTL8UqN8P/8BLt//z07jr/z0JzuNv//6a8cee+M5p3eqfei12zqNP9A580uNMvxSowy/1KiuS3R/PskTSR5PcnOS+X01Jmmyxg5/kqXAZ4EVVXU8MAc4t6/GJE1W19P+ucDbk8wFFgAvdm9J0hC6rNX3U+BK4HlgC/CLqrp3+nEu0S3NTl1O+xcDq4GjgCOAhUnOn36cS3RLs1OX0/6PAj+pqm1VtQO4DTi1n7YkTVqX8D8PnJxkQZIwtUT3xn7akjRpXR7zrwduBR4Gfjj6Wtf01JekCeu6RPcXgS/21IukAfkKP6lRhl9qlG/p1R7Nu2fDTLegCXHmlxpl+KVGGX6pUYZfapThlxpl+KVGGX6pUYZfapThlxpl+KVGGX6pUYZfapThlxpl+KVGGX6pUb6fX3v04388udP4V17//thjD3v0fzrVrk6jD3zO/FKjDL/UKMMvNWqv4U9yfZKtSR7fbd+SJGuTPDO6XTzZNiX1bV9m/m8Cq6btuxRYV1XHAOtG25L2I3sNf1V9F3h52u7VwA2j+zcA5/Tcl6QJG/cx/3uqagvA6Pbdb3WgS3RLs9PEL/i5RLc0O40b/p8lORxgdLu1v5YkDWHc8N8BrBndXwN8u592JA1lX57quxl4ADg2yeYkFwD/AHwsyTPAx0bbkvYje31tf1Wd9xafWtlzL5IG5Cv8pEYZfqlRvqV3P/Drc04ae+xrF/y8U+0NH7iq0/hVf3Px2GMXbXigU23tmTO/1CjDLzXK8EuNMvxSowy/1CjDLzXK8EuNMvxSowy/1CjDLzXK8EuNMvxSowy/1CjDLzXK8EuN8v38+2jO4vFXJDvz3zd1qr183j+PPfZvN57dqfbHz/urTuMXfc/35M9WzvxSowy/1CjDLzVq3CW6r0jyVJLHktyeZNFk25TUt3GX6F4LHF9V7weeBi7ruS9JEzbWEt1VdW9V7Rxt/gBYNoHeJE1QH4/5PwXc3cPXkTSgTs/zJ7kc2AnctIdjLgQuBJjPgi7lJPVo7PAnWQOcBaysqnqr46rqGuAagEOy5C2PkzSsscKfZBXwBeCPq+pX/bYkaQjjLtH9T8A7gbVJHk3y1Qn3Kaln4y7R/fUJ9CJpQL7CT2qU4Zca5Vt699Hrr7wy9tg7jju0Y/Xxxx/G0x1r60DlzC81yvBLjTL8UqMMv9Qowy81yvBLjTL8UqMMv9Qowy81yvBLjTL8UqMMv9Qowy81yvBLjTL8UqOyh3+823+xZBvwX3s45DDgvwdqx9rWPhBrv7eq3rUvBw4a/r1J8mBVrbC2ta09eZ72S40y/FKjZlv4r7G2ta09jFn1mF/ScGbbzC9pILMi/ElWJflRkk1JLh2w7pFJ7k+yMckTSS4aqvZuPcxJ8kiSOweuuyjJrUmeGn3/pwxY+/Ojn/fjSW5OMn/C9a5PsjXJ47vtW5JkbZJnRreLB6x9xejn/liS25MsmkTtvZnx8CeZA3wFOB04DjgvyXEDld8JXFxVvw+cDPz1gLV3uQjYOHBNgC8D91TV+4A/GKqHJEuBzwIrqup4YA5w7oTLfhNYNW3fpcC6qjoGWDfaHqr2WuD4qno/8DRw2YRq79GMhx84CdhUVc9W1XbgFmD1EIWraktVPTy6/0umArB0iNoASZYBZwLXDVVzVPcQ4MOM1lysqu1V9fMBW5gLvD3JXGAB8OIki1XVd4GXp+1eDdwwun8DcM5Qtavq3qraOdr8AbBsErX3ZjaEfynwwm7bmxkwgLskWQ6cAKwfsOyXgEuA3w1YE+BoYBvwjdFDjuuSLByicFX9FLgSeB7YAvyiqu4dovY076mqLaOetgDvnoEeAD4F3D0ThWdD+PMm+wZ9CiLJO4BvAZ+rqlcHqnkWsLWqHhqi3jRzgROBq6vqBOA1Jnfa+3+MHluvBo4CjgAWJjl/iNqzTZLLmXroedNM1J8N4d8MHLnb9jImfBq4uyQHMRX8m6rqtqHqAh8Czk7yHFMPdU5LcuNAtTcDm6tq11nOrUz9MRjCR4GfVNW2qtoB3AacOlDt3f0syeEAo9utQxZPsgY4C/izmqHn22dD+DcAxyQ5Ksk8pi7+3DFE4SRh6nHvxqq6aoiau1TVZVW1rKqWM/U931dVg8yAVfUS8EKSY0e7VgJPDlGbqdP9k5MsGP38VzIzFzzvANaM7q8Bvj1U4SSrgC8AZ1fVr4aq+wZVNeMfwBlMXfX8MXD5gHX/iKmHGI8Bj44+zpiB7/8jwJ0D1/wA8ODoe/9XYPGAtf8OeAp4HPgX4OAJ17uZqesLO5g667mAqaWP1wHPjG6XDFh7E1PXuXb9zn116N+5qvIVflKrZsNpv6QZYPilRhl+qVGGX2qU4ZcaZfilRhl+qVGGX2rU/wLwx4HbATfSEwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchvision.transforms import ToPILImage\n",
    "import matplotlib.pyplot as plt\n",
    "to_pil = ToPILImage()\n",
    "\n",
    "Im1 = train_input[0][0] # pour vérifier on prend la 102ème au pif\n",
    "Im2 = train_input[0][1] # pour vérifier on prend la 102ème au pif\n",
    "plt.figure()\n",
    "plt.imshow(Im1) # tada !\n",
    "plt.figure()\n",
    "plt.imshow(Im2) # tada !\n",
    "plt.figure()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
