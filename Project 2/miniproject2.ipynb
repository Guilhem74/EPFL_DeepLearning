{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-539acd0b83d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;32mFalse\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled( False )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3 Project 2 – Mini deep-learning framework\n",
    "\n",
    "The objective of this project is to design a mini “deep learning framework” using only pytorch’s\n",
    "tensor operations and the standard math library, hence in particular without using autograd or the\n",
    "neural-network modules.\n",
    "\n",
    "3.1 Objective\n",
    "\n",
    "Your framework should import only torch.empty, and use no pre-existing neural-network python\n",
    "toolbox. Your code should work with autograd globally off, which can be achieved with\n",
    "torch . set˙grad˙enabled ( False )\n",
    "Your framework must provide the necessary tools to:\n",
    "     build networks combining fully connected layers, Tanh, and ReLU,\n",
    "     run the forward and backward passes,\n",
    "     optimize parameters with SGD for MSE.\n",
    "You must implement a test executable named test.py that imports your framework and\n",
    " Generates a training and a test set of 1, 000 points sampled uniformly in [0, 1]2, each with a label 0 if outside the disk of radius 1/√2π and 1 inside,\n",
    " builds a network with two input units, two output units, three hidden layers of 25 units,2 of 3\n",
    " trains it with MSE, logging the loss,\n",
    " computes and prints the final train and the test errors.\n",
    "\n",
    "3.2 Suggested structure\n",
    "You are free to come with any new ideas you want, and grading will reward originality. The suggested simple structure is to define a class\n",
    "class Module ( object ) :\n",
    "    def forward ( self , * input ) :\n",
    "        raise NotImplementedError\n",
    "    def backward ( self , * gradwrtoutput ) :\n",
    "        raise NotImplementedError\n",
    "    def param ( self ) :\n",
    "        return []\n",
    "   and to implement several modules and losses that inherit from it.\n",
    "Each such module may have tensor parameters, in which case it should also have for each a similarly sized tensor gradient to accumulate the gradient during the back-pass, and\n",
    " forward should get for input, and returns, a tensor or a tuple of tensors.\n",
    " backward should get as input a tensor or a tuple of tensors containing the gradient of the loss with respect to the module’s output, accumulate the gradient wrt the parameters, and return a tensor or a tuple of tensors containing the gradient of the loss wrt the module’s input.\n",
    " param should return a list of pairs, each composed of a parameter tensor, and a gradient tensor of same size. This list should be empty for parameterless modules (e.g. ReLU). Some modules may requires additional methods, and some modules may keep track of information from the forward pass to be used in the backward.\n",
    "You should implement at least the modules Linear (fully connected layer), ReLU, Tanh, Sequential to combine several modules in basic sequential structure, and LossMSE to compute the MSE loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module ( object ) :\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        \n",
    "    def forward ( self , * input ):\n",
    "        a = x.mm(w)\n",
    "        s = ReLu(a)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def backward ( self , * gradwrtoutput ):\n",
    "        grad_w = 2* (y_pred - y)\n",
    "        w = w - lr * grad_w\n",
    "    \n",
    "        \n",
    "    def param ( self ):\n",
    "        return []\n",
    "    \n",
    "    \n",
    "    def sgd():\n",
    "        batch = next_training_batch(data, self.batch_size)\n",
    "        L = LossMSE(y_pred,y)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(object):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = torch.Tensor(size=(self.in_features,self.out_features ))\n",
    "        \n",
    "    def print_(self):\n",
    "        print(self.weight)\n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential():\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "        for linear in kwargs['args']:\n",
    "            linear.print_()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.9445e+25,  5.6472e-43,  1.3816e+31],\n",
      "        [ 1.9203e+31,  6.3015e+34,  6.2688e+22],\n",
      "        [ 4.7428e+30,  1.5975e-43,  4.3204e-05]])\n",
      "tensor([[-1.8318e+24,  5.6472e-43,  8.5305e+02],\n",
      "        [ 7.3123e+28,  6.8945e+34,  8.5771e-39],\n",
      "        [ 2.1019e-44,  3.6013e-43,  4.3204e-05]])\n",
      "tensor([[-1.1591e+24,  5.6472e-43,  2.7708e+20],\n",
      "        [ 2.2945e+02,  1.9791e+05,  3.9586e+12],\n",
      "        [ 1.0333e+18,  0.0000e+00,  0.0000e+00]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Sequential at 0x193824369e8>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sequential(args = (Linear(3,3),Linear(3,3),Linear(3,3)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossMSE(object):\n",
    "    def __init__(self, ):\n",
    "        object.__init__(self)\n",
    "        loss = (y_pred - y).pow(2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLu(object):\n",
    "    \n",
    "    def __init__(self, ):\n",
    "        object.__init__(self)\n",
    "        \n",
    "    def do(self,x):\n",
    "        y = x.clamp(min = 0)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(object):\n",
    "    def __init__(self, ):\n",
    "        object.__init__(self)\n",
    "        \n",
    "    def do(self,x):\n",
    "        y = (math.exp(x) - math.exp(-x)) / ( math.exp(x) +  math.exp(-x))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_disc_set(nb):\n",
    "    input = Tensor(nb, 2).uniform_(-1, 1)\n",
    "    target = input.pow(2).sum(1).sub(2 / math.pi).sign().add(1).div(2).long()\n",
    "    return input, target\n",
    "\n",
    "train_input, train_target = generate_disc_set(1000)\n",
    "test_input, test_target = generate_disc_set(1000)\n",
    "\n",
    "mean, std = train_input.mean(), train_input.std()\n",
    "\n",
    "train_input.sub_(mean).div_(std)\n",
    "test_input.sub_(mean).div_(std)\n",
    "\n",
    "\n",
    "\n",
    "mini_batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4., 5.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Linear at 0x19382411d30>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
